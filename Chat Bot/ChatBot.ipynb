{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ChatBot.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vxydFqb9p9-i"
      },
      "source": [
        "## Import **Library**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RND5yAhXqDii"
      },
      "source": [
        "import numpy as np\n",
        "import nltk\n",
        "import string\n",
        "import random"
      ],
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BGYlDX2eqPo4"
      },
      "source": [
        "## Upload a file to colab"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BoRIkvEFqNdM"
      },
      "source": [
        "# once you are done just comment it out\n",
        "# from google.colab import files\n",
        "# uploaded = files.upload()"
      ],
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7KQ8RBH9qZKD"
      },
      "source": [
        "# Reading the **Corpus**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kyF--82xqUVR"
      },
      "source": [
        "f = open('chatbot.txt','r',errors='ignore')\n",
        "raw_docs = f.read() # read the content from file handler\n",
        "raw_docs = raw_docs.lower() # convert everything into lower case"
      ],
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wGzQhJnOq84u",
        "outputId": "5239a83d-15af-4974-f0ed-5d7b6675bfc9"
      },
      "source": [
        "# tokenization\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n"
      ],
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 87
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JUzJzr5yrHAV"
      },
      "source": [
        "sent_tokens = nltk.sent_tokenize(raw_docs) # convert entire file into sentence\n",
        "word_tokens = nltk.word_tokenize(raw_docs) # convert sentences into list of words"
      ],
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jZO4L-NarZXo",
        "outputId": "b5a39e36-b3f6-4597-9e64-39fa87ad885f"
      },
      "source": [
        "sent_tokens[:2]"
      ],
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['data science\\nfrom wikipedia, the free encyclopedia\\njump to navigationjump to search\\nnot to be confused with information science.',\n",
              " 'the existence of comet neowise (here depicted as a series of red dots) was discovered by analyzing astronomical survey data acquired by a space telescope, the wide-field infrared survey explorer.']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 89
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bl5gpjdmrkxE",
        "outputId": "80d18ce2-d5eb-40c7-ab5a-ac231cb0ecfe"
      },
      "source": [
        "word_tokens[:5]"
      ],
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['data', 'science', 'from', 'wikipedia', ',']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 90
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6jLvZdmlrtzg"
      },
      "source": [
        "# Text Preprocessing "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GERDklgeryAs"
      },
      "source": [
        "# lemmatization\n",
        "lemmer = nltk.stem.WordNetLemmatizer()"
      ],
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3lkGlcZWsAp0"
      },
      "source": [
        "def lemToken(tokens):\n",
        "  return [lemmer.lemmatize(token) for token in tokens]"
      ],
      "execution_count": 92,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BfZjDp9lsQLF"
      },
      "source": [
        "# remove punctuations\n",
        "remove_punc = dict((ord(punc),None) for punc in string.punctuation)"
      ],
      "execution_count": 93,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xUuS8fHcshTk"
      },
      "source": [
        "def lemNormalize(text):\n",
        "  return lemToken(nltk.word_tokenize(text.lower().translate(remove_punc)))"
      ],
      "execution_count": 94,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fxvQAD0Bt38N"
      },
      "source": [
        "# Greeting Function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vqZdUhPnt0gJ"
      },
      "source": [
        "greet = (\"Hello\",\"Hey\",\"Hi\",\"hi\",\"hiii\",\"Hiii\",\"Wasup\",\"Whats up\",\"Hey, How you doing?\",\"sup\",\"supppp\")\n",
        "greet_responce = [\"Hello\", \"Hey\",\"Hi\",\"Hi, am good. how about you\",\"hmm listening\"]\n",
        "\n",
        "\n",
        "def greeting(sentence):\n",
        "  for word in sentence.split():\n",
        "    if word.lower() in greet:\n",
        "      return random.choice(greet_responce) # gives a random greeting"
      ],
      "execution_count": 95,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x7_AmMmMvEcV"
      },
      "source": [
        "# Response Generation\n",
        "### **TFIDF**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_R2HERX4uzBY"
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity"
      ],
      "execution_count": 96,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wvifWAEsx32D"
      },
      "source": [
        "def respose(user_response):\n",
        "  respose = ''\n",
        "  tfidf_Vec = TfidfVectorizer(tokenizer=lemNormalize, stop_words='english')\n",
        "  tfidf = tfidf_Vec.fit_transform(sent_tokens)\n",
        "  # print(tfidf)\n",
        "  vals = cosine_similarity(tfidf[-1],tfidf)\n",
        "  idx = vals.argsort()[0][-2]\n",
        "  flat = vals.flatten()\n",
        "  flat.sort()\n",
        "  req_tfidf = flat[-2]\n",
        "  if (req_tfidf == 0):\n",
        "    respose = respose + 'Sorry, I quite didnt get you'\n",
        "    return respose\n",
        "  else:\n",
        "    respose = respose + sent_tokens[idx]\n",
        "    return respose\n"
      ],
      "execution_count": 97,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U9Ss3ENhzGLM"
      },
      "source": [
        "# Defining Start and End Protocol"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ehy63rTiyV0S",
        "outputId": "52d0d5c6-f35f-4160-d2e5-a96318ed249a"
      },
      "source": [
        "flag = True\n",
        "print(\"Bot: Hello, How can I help you? , If you want to exit at any time, Just say Bye :c \")\n",
        "while flag == True:\n",
        "  user_respose = input()\n",
        "  user_respose = user_respose.lower()\n",
        "  if user_respose != 'bye':\n",
        "    if user_respose == 'thank you' or user_respose == 'thanks':\n",
        "      flag = False\n",
        "      print(\"Bot: Pleasure was mine\")\n",
        "    else:\n",
        "      if greeting(user_respose) != None :\n",
        "        print(\"Bot: \"+greeting(user_respose))\n",
        "      else:\n",
        "        sent_tokens.append(user_respose)\n",
        "        word_tokens = word_tokens+nltk.word_tokenize(user_respose)\n",
        "        final_words = list(set(word_tokens))\n",
        "\n",
        "        print(\"Bot: \",end = '')\n",
        "        print(respose(user_respose))\n",
        "        \n",
        "        sent_tokens.remove(user_respose)\n",
        "\n",
        "  else:\n",
        "    flag = False\n",
        "    print(\"Bot: Bye. see you soon <3 \")"
      ],
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Bot: Hello, How can I help you? , If you want to exit at any time, Just say Bye :c \n",
            "hi\n",
            "Bot: hmm listening\n",
            "foundation of data\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
            "  'stop_words.' % sorted(inconsistent))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Bot: [4][5]\n",
            "\n",
            "\n",
            "contents\n",
            "1\tfoundations\n",
            "1.1\trelationship to statistics\n",
            "2\tetymology\n",
            "2.1\tearly usage\n",
            "2.2\tmodern usage\n",
            "3\timpact\n",
            "4\ttechnologies and techniques\n",
            "4.1\ttechniques\n",
            "5\treferences\n",
            "foundations\n",
            "data science is an interdisciplinary field focused on extracting knowledge from data sets, which are typically large (see big data), and applying the knowledge and actionable insights from data to solve problems in a wide range of application domains.\n",
            "impact of dara\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
            "  'stop_words.' % sorted(inconsistent))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Bot: \"the impacts of big data that you may not have heard of\".\n",
            "impact of data\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
            "  'stop_words.' % sorted(inconsistent))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Bot: \"the impacts of big data that you may not have heard of\".\n",
            "bye\n",
            "Bot: Bye. see you soon <3 \n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}