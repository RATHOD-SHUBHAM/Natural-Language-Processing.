# -*- coding: utf-8 -*-
"""Topic Modelling-LDA.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/11PZZZ1r6IgEANnduCQ1KUJags2rQVY-m

# Task :


# Given the abstract and title for a set of research articles, predict the topics for each article included in the test set.

## Steps:
1. Read the data from csv file.    
[Pandas](https://pandas.pydata.org/docs/user_guide/io.html)
2. Data Preparation
3. Extract the required information from the data frame.
4. Word Embedding.  
* [CountVectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html)
5. PreProcessing Data
* [WordNetLemmatizer](https://www.geeksforgeeks.org/python-lemmatization-with-nltk/)
6. Latent Dirichlet Allocation
* [LDA](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.LatentDirichletAllocation.html)

# Import Libraries
"""

# Commented out IPython magic to ensure Python compatibility.
# pandas
import pandas as pd

# sklearn - count vectorizers
from sklearn.feature_extraction.text import CountVectorizer

# LDA
from sklearn.decomposition import LatentDirichletAllocation

# Numpy
import numpy as np

# matplotlib
import matplotlib.pyplot as plt

# %matplotlib inline

# seaborn
import seaborn as sns

# nltk
import nltk

"""# step 1 : Read Data from csv file

[df.size, df.shape and df.ndim](https://www.geeksforgeeks.org/python-pandas-df-size-df-shape-and-df-ndim/)

[pandas.DataFrame.size](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.size.html)

[pandas.DataFrame.shape](https://pandas.pydata.org/pandas-docs/version/0.23/generated/pandas.DataFrame.shape.html)
"""

df_train = pd.read_csv("train.csv")
# print(df_train)

df_train.info()

df_train.shape

df_train.size

"""## Test data"""

df_test = pd.read_csv("test.csv")
# print(df_test)

df_test.info()

df_test.shape

df_test.size

"""# Step 2: Data Preparation

# **df_train**


1.   ID
2.   Title
3.   Abstract
4.   Subject it may belong to :
        *   Computer Science
        *   Physics
        *   Maths
        *   Stats
        *   Quantitative Biology	
        *   Quantitative Finance
"""

df_train.head()

"""# **df_test**

1.   ID
2.   Title
3.   Abstract


"""

df_test.head()

"""### select everything 

*   X = df_train.iloc[:]

### select everything from row 3 to end


*   x = df_train.iloc[3:]


[pandas.DataFrame.iloc](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.iloc.html)

### Check: 
* Total number of articles. 
* Total number of articles without label. 
* Total labels.
"""

x = df_train.iloc[:,3:].sum()
rowsum = df_train.iloc[:,2:].sum(axis = 1)
no_of_label_count = 0

for sum in rowsum.items():
  if sum == 0:
    no_of_label_count += 1

print("Total number of articles = ",len(df_train))
print("Total number of articles without label = ",no_of_label_count)
print("total labels = ",x.sum())

"""### Check for missing value"""

train_null = df_train.isnull().sum()
print("Number of missing value in train data: ",train_null)

test_null = df_test.isnull().sum()
print("Number of missing value in test data: ", test_null)


print("\n")
print("train null data: ",train_null.sum())
print("test null data: ",test_null.sum())

# for every row select from column 3 to end
# X = df_train.iloc[:,3:]
# X

# X = df_train.iloc[:,3:].sum() # column wise sum
# X

"""# Plot a Graph to look at he distribution

*  [Plotting with categorical variables](https://matplotlib.org/stable/tutorials/introductory/pyplot.html)

*  [How to use Seaborn Data Visualization for Machine Learning](https://machinelearningmastery.com/seaborn-data-visualization-for-machine-learning/)


*  [matplotlib.patches.Rectangle
](https://matplotlib.org/stable/api/_as_gen/matplotlib.patches.Rectangle.html)

*  [seaborn](https://seaborn.pydata.org/tutorial.html)

*  [How can I display text over columns in a bar chart in matplotlib
](https://stackoverflow.com/questions/7423445/how-can-i-display-text-over-columns-in-a-bar-chart-in-matplotlib)
"""

'''
x.index =  Index(['Computer Science', 'Physics', 'Mathematics', 'Statistics',
       'Quantitative Biology', 'Quantitative Finance'],
      dtype='object')


x.values = array([8594, 6013, 5618, 5206,  587,  249])


type(x.values) = numpy.

# create patch/rectangle for CS, Physics, Math, Stat, Quantitaive Bio, Quantitative Finacne
ax.patches = 
[<matplotlib.patches.Rectangle at 0x7f6b3bc080d0>,
 <matplotlib.patches.Rectangle at 0x7f6b3bbee110>,
 <matplotlib.patches.Rectangle at 0x7f6b3bbeec10>,
 <matplotlib.patches.Rectangle at 0x7f6b3bc08b50>,
 <matplotlib.patches.Rectangle at 0x7f6b3bb8f0d0>,
 <matplotlib.patches.Rectangle at 0x7f6b3bb8f510>]



rects = ax.patches
labels = x.values
for rect,label in zip(rects,labels):
  print(rect , label)


op:
Rectangle(xy=(-0.4, 0), width=0.8, height=8594, angle=0) 8594
Rectangle(xy=(0.6, 0), width=0.8, height=6013, angle=0) 6013
Rectangle(xy=(1.6, 0), width=0.8, height=5618, angle=0) 5618
Rectangle(xy=(2.6, 0), width=0.8, height=5206, angle=0) 5206
Rectangle(xy=(3.6, 0), width=0.8, height=587, angle=0) 587
Rectangle(xy=(4.6, 0), width=0.8, height=249, angle=0) 249

'''

"""##  display text over columns in a bar chart in matplotlib"""

def autolabel(rects,labels):
# attach some text labels
  for rect,label in zip(rects,labels):
      height = rect.get_height()
      plt.text(rect.get_x()+rect.get_width()/2., 1.02*height, label,
              ha='center', va='bottom')
  return

def graph_viz(x,title,label):
  sns.set_theme(style="whitegrid")
  plt.figure(figsize=(12,12))
  ax = sns.barplot(x.index, x.values, alpha = 0.8) # adjust transparency (alpha) 
  plt.title(title,fontsize = 12)
  plt.ylabel(label,fontsize = 12)
  plt.xlabel('Label',fontsize=12)

  # Display count in each class over columns in a bar chart
  rects = ax.patches
  labels = x.values
  autolabel(rects,labels)

  plt.show()

"""# dataset is highly imbalanced

# Number of occurance of each document
"""

x = df_train.iloc[:,3:].sum()
graph_viz(x,title = 'Class Count',label='# of occurance')

"""# No of Multiple tags per article"""

rowsum = df_train.iloc[:,2:].sum(axis = 1)
x = rowsum.value_counts() # Return a Series containing counts of unique values.
graph_viz(x,title = 'Multiple tags per article', label = '# of occurances')

"""# Step 3: Getting out only the abstract field"""

#df_test['ABSTRACT']

train_text = df_train['ABSTRACT']
print(train_text.head())
print('--------------------------')
test_text = df_test['ABSTRACT']
print(test_text.head())

# [df.value](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.values.html)
test_text.values

"""# Step 4:  Word Embedding

### **Function to get top n words**
[np.sum](https://numpy.org/doc/stable/reference/generated/numpy.sum.html)

[numpy.flip()](https://www.w3resource.com/numpy/manipulation/flip.php)

[numpy.argsort](https://numpy.org/doc/stable/reference/generated/numpy.argsort.html)

[numpy.sort](https://numpy.org/doc/stable/reference/generated/numpy.sort.html)

[numpy.zeros](https://numpy.org/doc/stable/reference/generated/numpy.zeros.html)

[pandas.DataFrame.values](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.values.html)

[pandas.DataFrame.iloc](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.iloc.html)

[Matplotlib.axes.Axes.bar()](https://www.geeksforgeeks.org/matplotlib-axes-axes-bar-in-python/)


[How vectorizer fit_transform work in sklearn](https://stackoverflow.com/questions/47898326/how-vectorizer-fit-transform-work-in-sklearn/54350840)

**Explanation to get the top n words from the test data:**
```
On test data perform vectorization by removing stop words (vectorization is converting text to numbers)

Then count the number of times each word has appeared(np.sum) since it is a vector. we count column wise. 

  a | b | c | d
0
1
2
3
------------------
sum 
--------------------

Capture the sum and index. Egsum value 4547 might be present at index 147.

sort (np.argsort) the index where the value are present and store them in descending order.
sort(np.sort) the sum in descending(np.flip) order and store.


now create a 2d matrix and fill it with zero

Now replace zero with 1 in the place(row, col).
column value is obtained from the index above.


once you replace the 0 with 1 convert the vector back into text

```
"""

# def get_top_n_words(n_top_word ,test_data):
#   vectorizer = CountVectorizer(stop_words = 'english') #initialize the vectorizer to remove stop words
#   transformed_vectorizer = vectorizer.fit_transform(test_data)
#   # print(transformed_vectorizer)
#   # print('-------------------------------')
#   # print('\n')
#   print(vectorizer.get_feature_names())
#   # print('-------------------------------')
#   # print('\n')
#   print(transformed_vectorizer.toarray())

#   vectorized_total = np.sum(transformed_vectorizer, axis = 0) # column wise addition
#   print("Vectorized total is: ",vectorized_total)
#   # print(np.argsort(vectorized_total))
#   # print(np.argsort(vectorized_total)[0,:])
#   print(np.flip(np.argsort(vectorized_total)[0,:]))
#   # print(np.flip(np.argsort(vectorized_total)[0,:],1))
#   # print(np.flip(np.sort(vectorized_total)[0,:],1))
#   print(np.flip(np.sort(vectorized_total)[0,:]))
#   # print(vectorized_total.shape[1]) # column
#   # print(vectorized_total.shape)

#   word_indices = np.flip(np.argsort(vectorized_total)[0,:],1)
#   print("word index is : ",word_indices)
#   word_value = np.flip(np.sort(vectorized_total)[0,:],1)
#   # print("Word Value is = ",word_value)
#   print("Word Value is = ",word_value[0,:n_top_word].tolist()[0])
  
#   # create a 2d array with 15 row and column and fill them with zero

#   word_vector = np.zeros((n_top_word,vectorized_total.shape[1]))

#   for i in range(n_top_word):
#     # print(i,word_indices[0,i])
#     word_vector[i,word_indices[0,i]] = 1

#   # print(word_vector)
#   # print(vectorizer.inverse_transform(word_vector))

#   inverse_vectorizer = vectorizer.inverse_transform(word_vector)
#   print("inverse of vector is : ",inverse_vectorizer)

#   # for word in inverse_vectorizer:
#   #   print(word)

#   words = [word[0] for word in inverse_vectorizer]
#   print(words)

#   return(words, word_value[0,:n_top_word].tolist()[0])

from sklearn.feature_extraction.text import TfidfVectorizer
def get_top_n_words(n_top_word ,test_data):
  vectorizer = TfidfVectorizer(stop_words = 'english') #initialize the vectorizer to remove stop words
  transformed_vectorizer = vectorizer.fit_transform(test_data)
  # print(transformed_vectorizer)
  # print('-------------------------------')
  # print('\n')
  print(vectorizer.get_feature_names())
  # print('-------------------------------')
  # print('\n')
  print(transformed_vectorizer.toarray())

  vectorized_total = np.sum(transformed_vectorizer, axis = 0) # column wise addition
  print("Vectorized total is: ",vectorized_total)
  # print(np.argsort(vectorized_total))
  # print(np.argsort(vectorized_total)[0,:])
  print(np.flip(np.argsort(vectorized_total)[0,:]))
  # print(np.flip(np.argsort(vectorized_total)[0,:],1))
  # print(np.flip(np.sort(vectorized_total)[0,:],1))
  print(np.flip(np.sort(vectorized_total)[0,:]))
  # print(vectorized_total.shape[1]) # column
  # print(vectorized_total.shape)

  word_indices = np.flip(np.argsort(vectorized_total)[0,:],1)
  print("word index is : ",word_indices)
  word_value = np.flip(np.sort(vectorized_total)[0,:],1)
  # print("Word Value is = ",word_value)
  print("Word Value is = ",word_value[0,:n_top_word].tolist()[0])
  
  # create a 2d array with 15 row and column and fill them with zero

  word_vector = np.zeros((n_top_word,vectorized_total.shape[1]))

  for i in range(n_top_word):
    # print(i,word_indices[0,i])
    word_vector[i,word_indices[0,i]] = 1

  # print(word_vector)
  # print(vectorizer.inverse_transform(word_vector))

  inverse_vectorizer = vectorizer.inverse_transform(word_vector)
  print("inverse of vector is : ",inverse_vectorizer)

  # for word in inverse_vectorizer:
  #   print(word)

  words = [word[0] for word in inverse_vectorizer]
  print(words)

  return(words, word_value[0,:n_top_word].tolist()[0])

# get the top n words from the text by converting them into vector
'''
function gets a tuple of top n words and their count.
'''
print(test_text)
# get_top_n_words(n_top_word = 15,test_data = test_text)
words, words_val = get_top_n_words(n_top_word = 15,test_data = test_text.values)

sns.set_theme(style="whitegrid")
fig,ax = plt.subplots(figsize=(12,12)) # this is needed when you are using axis.bar
ax.bar(words,words_val) 
ax.set_xticklabels(words, rotation='vertical') # makes the word vertical
plt.title('Top words excluding stop words',fontsize = 12)
plt.ylabel('# of occurance',fontsize = 12)
plt.xlabel('Words',fontsize=12)
plt.show()

"""# step 5: Topic Modelling

### Preprocessing Data - lemmatization

* [what does the min/max document frequency exactly means](https://stackoverflow.com/questions/27697766/understanding-min-df-and-max-df-in-scikit-countvectorizer)

* [use build analyzer](https://stackoverflow.com/questions/36182502/add-stemming-support-to-countvectorizer-sklearn)

add lemmatize support to CountVectorizer (sklearn)
"""

nltk.download('wordnet')

# import these modules
from nltk.stem import WordNetLemmatizer
  
lemmatizer = nltk.WordNetLemmatizer()
class CountVectorizer_lemmatizer(CountVectorizer):
  def build_analyzer(self):
    analyzer = super(CountVectorizer_lemmatizer, self).build_analyzer()
    return lambda doc:(lemmatizer.lemmatize(w) for w in analyzer(doc))

# train_text is  df_train['ABSTRACT']. all the characters from abstract.
# train_text
# print(list(train_text))
text = list(train_text)
# print(text)
# igonre a term that apperas in more than 95% of the document and ignore a term that appers in less than 2 document
tf_vectorizer = CountVectorizer_lemmatizer(decode_error='ignore', max_df = 0.95,min_df=2,stop_words='english') # applying count vectorizer
# print(tf_vectorizer)
tf = tf_vectorizer.fit_transform(text)
# print(tf)

"""#  Latent Dirichlet Allocation

[LSA Topic Modelling](https://gist.github.com/susanli2016/3f88f5aab3f844cc53a44817386d06ce)

* **Fit ()**

call fit() to train the model using the input training and data.

[fit() vs predict() vs fit_predict() in Python scikit-learn](https://towardsdatascience.com/fit-vs-predict-vs-fit-predict-in-python-scikit-learn-f15a34a8d39f)


In a nutshell: fitting is equal to training. Then, after it is trained, the model can be used to make predictions, usually with a .predict() method call.

[What does the “fit” method in scikit-learn do?](https://stackoverflow.com/questions/45704226/what-does-the-fit-method-in-scikit-learn-do)

LatentDirichletAllocation

[sklearn.decomposition.LatentDirichletAllocation](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.LatentDirichletAllocation.html)

[How to interpret LDA components (using sklearn)](https://stackoverflow.com/questions/35140117/how-to-interpret-lda-components-using-sklearn)
"""

# play around with this
n_topics = 10

lda = LatentDirichletAllocation(n_components = n_topics, max_iter=5, learning_method='online',learning_offset=50., random_state=0)

lda.fit(tf)

# making LDA TOP MATRIX USING CORPUS TF
lda_topic_modelling = lda.fit_transform(tf)

lda_topic_modelling

"""### return an integer list of predicted topic catergories for a given topic matrix

* [numpy.argmax](https://numpy.org/doc/stable/reference/generated/numpy.argmax.html)
"""

def get_keys(topic_matrix):
  # print(topic_matrix.argmax(axis = 1)) # axis = 1, will return maximum index in that array 
  keys = topic_matrix.argmax(axis = 1).tolist()
  print("length of the keys is: ",len(keys))
  return keys

"""### Return a tuple of topic categories and their accompanying magnitude for a given list of keys

[Counter](https://pymotw.com/2/collections/counter.html)
"""

from collections import Counter
def key_to_count(keys):
  count_pairs = Counter(keys).items()
  # print("Count_pairs",count_pairs)
  categories = [pair[0] for pair in count_pairs]
  # print("categories",categories)
  counts = [pair[1] for pair in count_pairs]
  # print("Counts: ",counts)
  return (categories, counts)

"""### **Getting category and count from LDA Model**"""

lda_keys = get_keys(lda_topic_modelling)
print("keys: ",lda_keys)
# key_to_count(lda_keys)

lda_categories, lda_count = key_to_count(lda_keys)

"""## **Return A list of n_topic strings, where each string contains the n most common words in a predicted category in order.**

[Code to returns a list of n_topic strings, where each string contains the n most common words in a predicted category, in order](https://gist.github.com/susanli2016/3f88f5aab3f844cc53a44817386d06ce)
"""

def get_top_n_words(n, keys, document_term_matrix, cv):
    top_word_indices = []

    for topic in range(n_topics): # go from 0 to 24
        temp_vector_sum = 0
        # print(len(keys))  # 20972
        # print(keys)
        for i in range(len(keys)): # go from 0 to 20971
            if keys[i] == topic:
                temp_vector_sum += document_term_matrix[i]
        
        
        temp_vector_sum = temp_vector_sum.toarray()
        # print(temp_vector_sum)

        # print(np.sort(temp_vector_sum))
        # print("\n")
        # print(np.argsort(temp_vector_sum))
        # print("\n")
        # print(np.argsort(temp_vector_sum)[0])
        # print("\n")
        # # last 15 col
        # print(np.argsort(temp_vector_sum)[0][-n:]) # from -15 th col to end for row 0
        # print("\n")
        # print(np.flip(np.argsort(temp_vector_sum)[0][-n:],0))
        # print("\n")
        # print("\n")


        top_n_word_indices = np.flip(np.argsort(temp_vector_sum)[0][-n:],0)
        top_word_indices.append(top_n_word_indices)  

    # print(document_term_matrix)
    # print(document_term_matrix.shape) #(20972, 24648)
    # print(document_term_matrix.shape[1]) #24648
    # print(np.zeros((1,document_term_matrix.shape[1])))
    # create one row with 24648 column
    # print(len(np.zeros((1,document_term_matrix.shape[1]))[0])) #24648

    top_words = []
    for topic in top_word_indices:
        topic_words = []

        for index in topic:
            temp_word_vector = np.zeros((1,document_term_matrix.shape[1]))
            temp_word_vector[:,index] = 1
            # print(temp_word_vector)
            
            the_word = cv.inverse_transform(temp_word_vector)[0][0]
            topic_words.append(the_word.encode('ascii').decode('utf-8'))
        top_words.append(" ".join(topic_words))         
    return top_words

n_top_words = 25
top_n_words = get_top_n_words(n_top_words, lda_keys, tf, tf_vectorizer)

"""**Printing word from each topic**"""

for i in range(len(top_n_words)):
    print("Topic {}: \n".format(i+1), top_n_words[i]+"\n")

"""t-Distributed Stochastic Neighbor Embedding is a non-linear dimensionality reduction algorithm used for exploring high-dimensional data.

[sklearn.manifold.TSNE](https://scikit-learn.org/0.15/modules/generated/sklearn.manifold.TSNE.html)

[numpy.vstack](https://numpy.org/doc/stable/reference/generated/numpy.vstack.html)

[numpy.mean](https://numpy.org/doc/stable/reference/generated/numpy.mean.html)
"""

# this will take time
from sklearn.manifold import TSNE
tnse_lda_model = TSNE(n_components=2, perplexity=50, early_exaggeration=4.0, learning_rate=100, n_iter=2000, metric='euclidean', init='random', verbose=0, random_state=0)
tnse_lda_vector = tnse_lda_model.fit_transform(lda_topic_modelling)

"""**# get top 5 words from each topic to use in plot**"""

k = 3
top_k_words_lda = get_top_n_words(k,lda_keys,tf,tf_vectorizer)
top_k_words_lda

"""**# returns a list of centroid vectors from each predicted topic category**"""

def get_mean_topic_vectors(keys, two_dim_vectors):
  mean_topic_vectors = []

  # n_topics = 10
  for i in range(n_topics):
    articles_in_the_topic = []

    for j in range(len(keys)):
      # print(i)
      # print(keys[j])
      if keys[j] == i:
        # print("two_dim_vectors[j]",two_dim_vectors[j])
        articles_in_the_topic.append(two_dim_vectors[j])

    articles_in_the_topic = np.vstack(articles_in_the_topic)
    # print("articles_in_the_topic",articles_in_the_topic)
    mean_article_in_that_topic = np.mean(articles_in_the_topic,axis = 0) # column wise
    # print("mean_article_in_that_topic",mean_article_in_that_topic)
    mean_topic_vectors.append(mean_article_in_that_topic)

  return mean_topic_vectors

"""**# Getting the mean of the topic vector for the visualization**"""

lda_mean_topic_vectors = get_mean_topic_vectors(lda_keys,tnse_lda_vector)

"""**Top word from each topic to use in plot**

[Bokeh](https://docs.bokeh.org/en/latest/docs/user_guide/annotations.html)

[Bokeh color_scatter](https://docs.bokeh.org/en/latest/docs/gallery/color_scatter.html)
"""

# Colourmap for the visualization
colormap = np.array([
    "#1f77b4", "#aec7e8", "#ff7f0e", "#ffbb78", "#2ca02c",
    "#98df8a", "#d62728", "#ff9896", "#9467bd", "#c5b0d5",
    "#c7c7c7", "#bcbd22", "#dbdb8d", "#17becf", "#9edae5",
    "#8c564b", "#c49c94", "#e377c2", "#f7b6d2", "#7f7f7f" ])
colormap = colormap[:n_topics]
print(colormap)

from bokeh.plotting import figure, output_file, show
from bokeh.models import Label

from bokeh.io import output_notebook
output_notebook()

# print(lda_mean_topic_vectors)
# print(tnse_lda_vector) # there are 2 column
# print("\n")
# print(tnse_lda_vector[:,0]) # first column
# print("\n")
# print(tnse_lda_vector[:,1]) # second Column
# print("\n")

plot = figure(title="t-SNE clustring of {} LDA topics".format(n_topics), plot_width=700, plot_height=700)
plot.scatter(x = tnse_lda_vector[:,0], y = tnse_lda_vector[:,1], color = colormap[lda_keys])


for i in range(n_topics):
  label = Label(x = lda_mean_topic_vectors[i][0], y = lda_mean_topic_vectors[i][1], text = top_k_words_lda[i], text_color = colormap[i])
  plot.add_layout(label)



show(plot)

"""# **Assigning Topics to document**"""

list(train_text)

doc_topic = lda.transform(tf)

# check for 20 documents
for n in range(20):
  # print(doc_topic[n])
  topic_most_pr = doc_topic[n].argmax()
  # print(topic_most_pr)
  print("Document #{} - topic: {}\n".format(n,topic_most_pr))
  # this means document 1 belongs to topic 5 which seems to be like Quantitative Biology

"""# Advanced Analysis of LDA
Main goal is to find the most relevant articles for each topic so they can be used as the links for further research
"""

# making a dataframe from the document-topic matrix
doc_topic_df = pd.DataFrame(data=doc_topic)
doc_topic_df

"""# printing the top 'n' articles for each topic"""

for (columnName, columnData) in doc_topic_df.iteritems():
  print(columnName, columnData)

for (columnName, columnData) in doc_topic_df.iteritems():
  print(columnData.values) # first column, second column and so on.
  print("\n")
  print(pd.DataFrame(data=columnData.values).sort_values(by=0, ascending=False)) # sort row wise. in descending order

for (columnName, columnData) in doc_topic_df.iteritems():
    n = 5
    print('Topic #', columnName)
    sorted_topic = pd.DataFrame(data=columnData.values).sort_values(by=0, ascending=False)
    sorted_topic.columns = [columnName]
    print(sorted_topic[:n])
    
    # store IDs and titles of top articles in a dataframe
    ids = sorted_topic[:n].index
    print("\n")

#plotting the distribution of documents over each topic
sns.set(rc={'figure.figsize':(10,5)})
doc_topic_df.idxmax(axis=1).value_counts().plot.bar(color='lightblue')

#store the distributions in a dataframe
distribution = doc_topic_df.idxmax(axis=1).value_counts()
distribution

"""
Reference: https://www.kaggle.com/rababazeem/topic-modeling-lda"""